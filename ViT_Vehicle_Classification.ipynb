{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Define dataset paths\n",
    "dataset_path = \"/content/drive/MyDrive/Vehicles_Datasets\"  # Original dataset\n",
    "output_path = \"/content/drive/MyDrive/Vehicles_Datasets_Split\"  # New split dataset\n",
    "train_ratio = 0.8  # 80% train, 20% validation\n",
    "\n",
    "# Create train and val directories\n",
    "train_dir = os.path.join(output_path, \"train\")\n",
    "val_dir = os.path.join(output_path, \"val\")\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over each category (class)\n",
    "for category in os.listdir(dataset_path):\n",
    "    category_path = os.path.join(dataset_path, category)\n",
    "    if os.path.isdir(category_path):  # Ensure it's a directory\n",
    "        images = os.listdir(category_path)\n",
    "        random.shuffle(images)  # Shuffle images\n",
    "\n",
    "        split_idx = int(len(images) * train_ratio)\n",
    "        train_images = images[:split_idx]\n",
    "        val_images = images[split_idx:]\n",
    "\n",
    "        # Create class subdirectories in train and val folders\n",
    "        train_category_dir = os.path.join(train_dir, category)\n",
    "        val_category_dir = os.path.join(val_dir, category)\n",
    "        os.makedirs(train_category_dir, exist_ok=True)\n",
    "        os.makedirs(val_category_dir, exist_ok=True)\n",
    "\n",
    "        # Copy images to train and val folders\n",
    "        for img in train_images:\n",
    "            shutil.copy(os.path.join(category_path, img), os.path.join(train_category_dir, img))\n",
    "        for img in val_images:\n",
    "            shutil.copy(os.path.join(category_path, img), os.path.join(val_category_dir, img))\n",
    "\n",
    "        print(f\"Category '{category}' split completed: {len(train_images)} train, {len(val_images)} val\")\n",
    "\n",
    "print(\"Dataset splitting complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class VehicleDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images, organized into class folders.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Get all image paths and corresponding labels\n",
    "        for label, category in enumerate(self.classes):\n",
    "            category_path = os.path.join(root_dir, category)\n",
    "            for image_name in os.listdir(category_path):\n",
    "                self.image_paths.append(os.path.join(category_path, image_name))\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")  # Open image and convert to RGB\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return {\"pixel_values\": image, \"label\": label}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224 for ViT\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),  # Normalize to [-1, 1]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset_path = \"/content/drive/MyDrive/Vehicles_Datasets_Split\"\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = VehicleDataset(root_dir=f\"{dataset_path}/train\", transform=transform)\n",
    "val_dataset = VehicleDataset(root_dir=f\"{dataset_path}/val\", transform=transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading ViT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "import torch\n",
    "\n",
    "# Load ViT model\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=len(train_dataset.classes)  # Number of classes in your dataset\n",
    ")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
    "        inputs = batch[\"pixel_values\"].to(device)\n",
    "        labels = torch.tensor(batch[\"label\"]).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs).logits\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / len(train_dataset)\n",
    "    print(f\"Epoch {epoch + 1}: Loss = {total_loss:.4f}, Accuracy = {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        inputs = batch[\"pixel_values\"].to(device)\n",
    "        labels = torch.tensor(batch[\"label\"]).to(device)\n",
    "\n",
    "        outputs = model(inputs).logits\n",
    "        correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "accuracy = correct / len(val_dataset)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save ViT Model into drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "# Recreate the processor with the same parameters as used during training\n",
    "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "# Save the processor to your model directory\n",
    "processor.save_pretrained(\"/content/drive/MyDrive/Trained_ViT_Model\")\n",
    "\n",
    "# model.save_pretrained(\"/content/drive/MyDrive/Trained_ViT_Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call model and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ViTForImageClassification, AutoImageProcessor\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, ImageDraw, ImageFont\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification, AutoImageProcessor\n",
    "import torch\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "from IPython.display import display\n",
    "\n",
    "# Load the trained model and processor\n",
    "model_path = \"/content/drive/MyDrive/Trained_ViT_Model\"\n",
    "model = ViTForImageClassification.from_pretrained(model_path)\n",
    "processor = AutoImageProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Move the model to device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Set the threshold for \"unknown\" classification\n",
    "confidence_threshold = 0.5  # Adjust based on your dataset and use case\n",
    "\n",
    "# Load and preprocess the image\n",
    "# image_path = \"/content/drive/MyDrive/Testing_Images/bike.jpg\"\n",
    "# image_path = \"/content/drive/MyDrive/Testing_Images/motorbike.jpg\"\n",
    "# image_path = \"/content/drive/MyDrive/Testing_Images/car.jpg\"\n",
    "# image_path = \"/content/drive/MyDrive/Testing_Images/airplane.jpg\"\n",
    "# image_path = \"/content/drive/MyDrive/Testing_Images/ship.jpg\"\n",
    "image_path = \"/content/drive/MyDrive/Testing_Images/human.jpg\" \n",
    "if not os.path.exists(image_path):\n",
    "    raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "# Ensure proper preprocessing using the same processor used for training\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Perform inference\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "print('\\n==========================[ Vehicles Classification using ViT model ]==========================\\n')\n",
    "\n",
    "# Get the raw logits (model outputs before applying softmax)\n",
    "logits = outputs.logits\n",
    "print(\"Logits:\", logits)  # Print raw logits\n",
    "\n",
    "# Apply softmax to get probabilities\n",
    "probabilities = torch.softmax(logits, dim=1)\n",
    "max_prob, predicted_class_index = torch.max(probabilities, dim=1)\n",
    "\n",
    "# Define class names (update with your own class labels)\n",
    "class_names = [\"cars\", \"ships\", \"motorbikes\", \"airplane\", \"bicycles\"]\n",
    "\n",
    "# Check confidence threshold\n",
    "if max_prob.item() < confidence_threshold:\n",
    "    predicted_label = \"unknown\"\n",
    "    predicted_class_index = -1  # Assign -1 for unknown classes\n",
    "else:\n",
    "    predicted_label = class_names[predicted_class_index.item()]\n",
    "    predicted_class_index = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "# Print the result\n",
    "print(f\"Predicted class index: {predicted_class_index}\")\n",
    "print(f\"Predicted class name: {predicted_label}\")\n",
    "print(f\"Confidence score: {max_prob.item()}\")\n",
    "\n",
    "# Save and display the labeled image\n",
    "output_image_path = \"/content/drive/MyDrive/Testing_Images/predicted_images.png\"\n",
    "image.save(output_image_path)\n",
    "print(f\"Labeled image saved at: {output_image_path}\")\n",
    "\n",
    "# Resize the image (for example, to 600x600)\n",
    "resized_image = image.resize((600, 600))\n",
    "\n",
    "# Display the image inline (for Jupyter or Colab)\n",
    "display(resized_image)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
